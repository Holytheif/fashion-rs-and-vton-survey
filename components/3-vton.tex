\section{Virtual try-on} \label{section:vton}
	Virtual try-On systems represent a cutting-edge fusion of technology and fashion retail, revolutionizing the way consumers interact with clothing online. These systems utilize artificial intelligence, computer vision, and augmented reality to simulate the experience of trying on clothing virtually. Users can see how garments fit, drape, and look on their own bodies without physically trying them on. From 2D image-based try-ons to advanced 3D models, these systems offer a range of immersive experiences.
	
	The goal of virtual try-on is not only to enhance user confidence and satisfaction, but also to reduce return rates, making it a pivotal tool in modern e-commerce. It showcases the potential of technology to bridge the gap between digital and physical retail, offering consumers an engaging and informative way to make fashion choices online.

	\subsection{Image-based (2D) virtual try-on}
		Image-based try-on systems take in images as input data and generate images as output data. The systems infer data like pose, warp, and occlusion of the target and then use that information to guide the generation of the post-try-on image.

		In \citeyear{DBLP:conf/iccvw/AyushJCHK19} \lithead{DBLP:conf/iccvw/AyushJCHK19}{
			present a multi-scale patch adversarial loss function that improves cloth fit and texture preservation in virtual try-on. A Geometric Matching Module (GMM) enhances control over shape and pose transformations, and adversarial loss promotes realistic texture and body shape propagation. Reformulation as a conditional image generation problem offers an economical virtual try-on solution without 3D information.
		} In another work \cite{DBLP:conf/iccvw/AyushJCK19}, they demonstrate how multi-task learning improves efficiency and task performance. Auxiliary tasks like semantic segmentation can enhance virtual try-on, and pseudo ground-truth masks aid in image fusion, leading to a better fit.
		
		\lithead[ClothFlow]{DBLP:conf/iccv/HanHHS19}{
			utilizes dense optical flow estimation for natural-looking clothing deformation and appearance transfer. The cascaded flow network ensures accurate appearance matching, enhancing clothed person generation and a conditional layout generator disentangles shape and appearance for spatially coherent results.
		} And in a similar vein, \lithead[UVTON]{DBLP:conf/iccvw/KuboISM19}{
			leverages UV mapping for accurate virtual try-on across diverse postures.
		} Both of these rely on DensePose integrations to enhance mapping precision by estimating 3D surface information for each pixel. However, DensePose accuracy is crucial for success and that is challenging as warping 2D image textures to predefined coordinate systems can introduce artifacts.

		\lithead[LA-VITON]{DBLP:conf/iccvw/LeeLKCP19}{
			excels in damage-free, visually appealing virtual try-on results by using a GMM and Try-On Module (TOM) to ensure seamless synthesis. A Grid interval consistency loss maintains shape and pattern consistency, and an occlusion-handling technique enhances geometric matching for superior results.
		} And \lithead[VTNFP]{DBLP:conf/iccv/YuWX19}{
			retains clothing and body details by leveraging a GAN \cite{DBLP:journals/corr/GoodfellowPMXWOCB14} and cGAN's \cite{DBLP:journals/corr/MirzaO14} powerful generative capabilities.
		}

		The use of StyleGAN \cite{DBLP:journals/pami/KarrasLA21} by \lithead{DBLP:conf/iccvw/YildirimJVB19}{
			enables high-resolution fashion model image generation, enhancing clothing shopping visualizations. Conditional StyleGAN with embedding network allows customization of outfits and poses for fashion models. Swapping style vectors in unconditional StyleGAN enhances flexibility in image generation. Further, deep pose estimator and keypoint extraction improve accuracy in representing body poses.
		}

		In \citeyear{minar2020cp} \lithead[CP-VTON+]{minar2020cp}{
			corrects clothing-agnostic human representation by addressing labeling and omission issues. It enhances composition masks using input clothing mask and a concrete loss function.
		}

		\lithead[WUTON]{DBLP:conf/eccv/IssenhuthMC20}{
			adopts a student-teacher paradigm to improve virtual try-on performance by exploiting masked information. Adversarial loss ensures the student model mimics real image distribution, enhancing realism. A Real-time virtual try-on is achieved by omitting human parser and pose estimator at inference.
		}

		A hybrid 2D-3D method proposed by \lithead{minar20203d}{
			enables accurate 3D clothing reconstruction for natural shapes. 3D deformations preserve clothing characteristics, especially for large transformations or detailed textures, and the template-based approach aligns clothing image with the silhouette of the body's 3D model for precise reconstruction. SMPL model ensures accurate representation of diverse body shapes and poses. An updated TOM network enhances blending stage for improved virtual try-on results.
		}

		In \citeyear{DBLP:conf/cvpr/LiCZL21} \lithead[OVNet]{DBLP:conf/cvpr/LiCZL21}{
			presents a method to capture important details like buttons, shading, textures, and realistic hemlines, resulting in high-quality virtual try-on images. It introduces a technique for matching outfits with the most suitable model, leading to significant improvements in try-on results. It also consists of a semantic layout generator and an image generation pipeline using multiple coordinated warps, which yields consistent improvements in detail. But it does not handle variations in body shape, which limits its ability to dress garments directly on different body types. It also does not address the challenge of obtaining 3D measurements of garments and users.
		}

		Yet another hybrid approach is proposed in \lithead[CloTH-VTON+]{DBLP:journals/access/MinarTA21}{
			which combines 2D and 3D methods for realistic clothing deformations while preserving details. 2D methods generate disclosed human parts, ensuring visually appealing try-on outputs, while 3D cloth reconstruction allows flexibility in applying the method to different clothing styles.
		}

		\lithead[DCTON]{DBLP:conf/cvpr/GeSGY0021}{
			can produce highly-realistic try-on images by disentangling important components of virtual try-on. It utilizes perceptual loss to ensure similar CNN feature representations between the warped clothes. It disentangles clothing, skin, and background components from input images. However, it does not differentiate clothing and non-clothing regions, which can hinder the quality of results.
		}

		Successful synthesis of high-resolution virtual try-on is achieved by \lithead[VITON-HD]{DBLP:conf/cvpr/ChoiPLC21}{} It introduces a misalignment-aware normalization technique to address spatial deformation and generate properly aligned images and utilizes conditional normalization layers to preserve semantic information and apply spatially varying affine transformations.

		Another student-teacher model, \lithead[PF-AFN]{DBLP:conf/cvpr/GeSZG0021}{
			produces highly photo-realistic try-on images without relying on human parsing by the use of appearance flows between the person image and the garment image which enables accurate dense correspondences and high-quality results. It introduces an adjustable distillation loss function to ensure accurate representations and predictions. However, heavily relies on the use of a parser-based model as the `teacher' network, which may limit the image quality of the `student' network.
		}

		\lithead[FashionMirror]{DBLP:conf/iccv/ChenLHSC21}{
			proposes a co-attention feature remapping framework for virtual try-on to generate realistic results with spatio-temporal smoothness. It allows for refinement through convolution and generating different views.
		} \lithead[ZFlow]{DBLP:conf/iccv/ChopraJHK21}{
			presents an end-to-end framework addressing concerns regarding geometric and textural integrity. It incorporates gated aggregation of hierarchical flow estimates (Gated Appearance Flow), and dense structural priors to improve depth perception, handling of occlusion, and reduction of artifacts in try-on outputs.
		}

		\lithead[DiOr]{DBLP:conf/iccv/CuiML21}{
			supports various fashion-related tasks such as 2D pose transfer, virtual try-on, and outfit editing. It explicitly encodes the shape and texture of each garment, allowing for separate editing of these elements. A joint training on pose transfer and inpainting helps with detail preservation and coherence of generated garments. However, complex or rarely seen poses may not always be rendered correctly, and ghosting artifacts and improper filling of holes in garments can also be present.
		}

		In \citeyear{DBLP:conf/cvpr/FenocchiMCBCC22} \lithead[DBCT]{DBLP:conf/cvpr/FenocchiMCBCC22}{
			improves the generation of virtual try-on results by dealing with long-range dependencies and achieving more realistic and accurate results and demonstrates its effectiveness in comparison to both standard pure convolutional approaches and previous transformer-based proposals.
		}

		A cheap and scalable weakly-supervised method is proposed by \lithead{DBLP:conf/cvpr/FengMSGLLOZZ22}{
			which shows that projecting the rough alignment of clothing and body onto the StyleGAN space can yield photo-realistic wearing results. The Deep Generative Projection algorithm has a time cost for semantic and pattern searches and the method still faces difficulty against state-of-the-art methods in handling extremely complicated poses. Has an undesirably high FID score of 48.4.
		}

		\lithead{DBLP:conf/cvpr/HeSX22}{
			presents a perceptual loss and a warping model to train the try-on model, which helps improve the quality of the generated appearance flow and the final try-on results. Usage of a pre-trained VGG network \cite{DBLP:journals/corr/SimonyanZ14a} and a parser-based model in the training process enhances the accuracy and effectiveness.
		}

		Accurate photorealistic results are shown by \lithead[RT-VTON]{DBLP:conf/cvpr/YangY022}{
			for both standard and non-standard clothes. Is able to handle hard samples such as off-shoulder clothes. Semi-rigid deformation technique used in the method balances the trade-off between rigidity and flexibility of clothes warping.
		}

		\lithead[SDAFAN]{DBLP:conf/eccv/BaiZLZY22}{
			enables clothing warping and body synthesizing simultaneously by extracting and merging feature and pixel-level information from different semantic areas. It addresses challenges without relying on intermediate parser-based labels, reducing noise and inaccuracies in the try-on process.
		} \lithead[HR-VTON]{DBLP:conf/eccv/LeeGPCC22}{
			ensures information exchange and eliminates misalignment and pixel-squeezing artifacts by filtering out incorrect segmentation map predictions.
		}

		\lithead[C-VTON]{DBLP:conf/wacv/FeleLPS22}{
			generates convincing results even with subjects in difficult poses, allowing for synthesis of high-quality try-on results. However, it has issues with the masking procedure when generating image context and loose clothing. Further, it lacks the ability to differentiate between the front and backside of the target garment.
		} \lithead[3D-GCL]{DBLP:conf/nips/HuangLXKCL22}{
			incorporates 3D parametric human models as priors to better handle variations of pose and viewpoint. It utilizes 3D-aware global correspondences that encode global semantic correlations.
		}

		In \citeyear{DBLP:conf/cvpr/ZhuYZRCS0K23} \lithead[TryOnDiffusion]{DBLP:conf/cvpr/ZhuYZRCS0K23}{
			generates apparel try-on results with significant body shape and pose modification by using a latent diffusion method \cite{DBLP:conf/cvpr/RombachBLEO22}. But it focuses on upper body clothing and does not experiment with full body try-on. Further, performance with more complex backgrounds is unknown.
		} \lithead[MGD]{DBLP:journals/corr/abs-2304-02051}{
			also takes this diffusion model approach and incorporates multimodal inputs such as text, body pose, and sketches and achieves good results in terms of realism and coherence with multimodal inputs. It also introduces new semi-automatically annotated datasets.
		}

		\lithead[SAL-VTON]{DBLP:conf/cvpr/YanGZX23}{
			introduces semantically associated landmarks for virtual try-on which helps in addressing misalignment issues and improving the try-on results. It also provides the ability to edit virtual try-on results using landmarks.
		} \lithead[GP-VTON]{DBLP:conf/cvpr/XieHDZDZ0L23}{
			preserves semantic information, avoids texture distortion, and handles challenging inputs by employing local flows to warp garment parts individually and dynamically truncating the gradient in the overlap area, effectively avoiding texture squeezing problems. It also easily extendeds to a multi-category scenarios.
		}

	\subsection{Pose-guided human synthesis}
		These systems are also image-based, but they focus on transforming a human image from reference to target pose while preserving style but changing clothing.

        In \citeyear{DBLP:conf/cvpr/SongZLM19} \lithead{DBLP:conf/cvpr/SongZLM19} {
            proposed a method that simplified non-rigid deformation learning by using semantic parsing transformation and appearance generation tasks and attempting end-to-end training which lead to refined results. Semantic generative network helped in transforming semantic parsing maps which in turn facilitates deformation learning. Appearance generative network synthesizes semantic-aware textures for realistic person images. 
        }

        For tasks like person image synthesis, sketch-to-photo and depth up sampling, \lithead{DBLP:conf/iccv/AlbaharH19}{
            proposed a conditioning scheme with diverse guidance signals and a bi-directional feature transformation for enhanced information flow.
        } 

        For realistic pose transformation with precise pixel transfer, 3D appearance flow is much better. \lithead{DBLP:conf/cvpr/LiHL19} {
            observed that scarce annotations can be overcome by fitting a 3D model and projecting to 2D for dense appearence flow computation. For accurate pose transfer, synthesized gropund-truths were able able to map target poses to 3D appearence flow leading to accurate pose transfer. Such workflow lead to photorealistic images of the target pose.
        }
		
		Two discriminators were used by \cite{DBLP:journals/corr/abs-1906-07251} to differentiate between real and generated images which ensured that the fashion item remains consistent during synthesis. Image and pose encoders provided crucial context for high-quality image generation and end-to-end training optimized generative model parameters for synthesis of photorealistic images.

        Personalized modelling and shape disentanglement was enabled by 3D body mesh recovery module. \cite{DBLP:conf/iccv/LiuPML0G19} realized that Liquid Warping GAN with Liquid Warping Block preserves vital details in both image and feature spaces and it also supported flexible warping from multiple sources which led to diverse results.

        In \citeyear{DBLP:conf/ijcai/HuangXCWZWHD20} \cite{DBLP:conf/ijcai/HuangXCWZWHD20} introduced adaptive patch normalization which improved performance through region specific normalization. It coupled target pose with conditioned appearance for realistic image generation. High flexibility was observed because of decoupling and recoupling of factors. 
        
        \cite{DBLP:conf/cvpr/MenMJML20} built an Attribute-Decomposed GAN which enabled precise control of attributes during image synthesis. The automatic separation of attributes removed manual annotations and led to enhanced realism. The dependency on labelled data, however, may limit applicability in situations lacking readily available annotations.

        GANs can be used to enhance the quality of images which is useful in realistic pose transfer. \lithead[PoNA]{DBLP:journals/tip/LiZLLD20}{
            has a long range dependency that addresses feature information gaps and self-occlusion issues. Integration of pre-posed image-guided pose feature update and post-posed image feature update improves feature utilization in the transfer process. The method generated sharper, detailed images with lesser parameters and faster speed which increased efficiency for pose transfer tasks. Struggles with invisible areas, complex poses because of simplified block structure and posing limitations in occlusion handling were observed. Reliance on GAN was expected to cause mode collapse or instability in certain scenarios and it also faced difficulty in preserving fine details and precise transfer of textures when there were significant coordinate differences.
        }

        \lithead[SPAdaIN]{DBLP:conf/cvpr/WangWFLZXZ20}{
            extends the already 2D SPADE \cite{DBLP:conf/cvpr/Park0WZ19} approach, enabling effective pose transfer. It works directly on identity meshes, bypassing need for specific additional information. SPAdaIN ResBlocks ensure high-quality output meshes with combined pose and identity information. Geometric details are maintained by edge regularization for enhanced results. Downside to this approach is it's reliance on high quality identity meshes and pose information may hinder practical use in certain scenarios.
        }
        
        Combination of flow-based operations and attention was observed by \cite{DBLP:journals/tip/RenLLL20}  to improve training stability and gradient propagation. It enables accurate feature-level spatial transformation, enhancing pose-guided image generation. Irrelevant info adversely the texture accuracy in the global attention model. 
		
        Enhanced accessibility and efficiency because of high-resolution appearance transfer without 3D model was attempted by \cite{DBLP:journals/corr/abs-2008-11898}. It utilized dense local descriptors for refined details and preserved garment textures and geometry. The progressive training in autoencoder improved generation quality at high resolutions. It realistically reproduces complex garment appearance, including occluded areas, for accurate transfer. However, global perceptual loss might not be able to preserve sharp garment textures, as noted in ablation study.

		\lithead{DBLP:journals/corr/abs-2006-01435}{
			simplified portrait editing and eliminated manual software use. It also allowed simultaneous revision of posture, body figure, and clothing style through use of a GAN-based method which maintained coherency and found to preserve identity in recaptured portraits. The layout map guided appearance transformation leading to enhanced accuracy. The hierarchical knowledge aided in effective recapture, especially for invisible parts.
		}

		In the year \citeyear{DBLP:journals/tog/AlBaharLYSSH21} \lithead{DBLP:journals/tog/AlBaharLYSSH21}{ developed conditional StyleGAN which enabled accurate pose-guided image synthesis. The Inpainted Correspondence Field facilitated detail transfer for drastic pose changes. The Spatially Varying Latent Space Modulation preserved local details and photo-realism.
		}

        A discussed earlier, GANs have been used to enhance image realism and shape consistency which in turn leads to appealing results, acting as high resolution data for training models. In \citeyear{DBLP:journals/pami/ZhuHXSCB22} \lithead[PATN]{DBLP:journals/pami/ZhuHXSCB22}{
            developed Pose-Attentional Transfer Network which allowed accurate and natural-looking pose transfers through intermediate representations. Pose distortions were reduced and image quality increased because of Perceptual L1 loss integration.
        }

		\lithead[wFlow]{DBLP:conf/cvpr/DongZXZDZLLY22}{
			is capable of transferring arbitrary garments onto challengingly-posed query person images in real-world backgrounds. It efficiently integrates the advantages of 2D pixel-flow and 3D vertex-flow. The self-supervised training scheme used in the paper leverages easily obtainable dance videos to train the model. However, since it focuses on garment transfer in the context of dance videos, it is unclear how well it performs on a large scale or with diverse datasets.
		}

		Controlled person image synthesis by blending source style with target pose was attempted by \lithead[CASD]{DBLP:conf/eccv/ZhouYCSGL22}{
            Self-attention aided accurate source appearance encoding which enhanced model effectiveness. Contextual loss was computed using VGG-19 features which facilitated image transformation.
        } 
		

		\lithead[InsetGAN]{DBLP:conf/cvpr/FruhstuckSSMWL22}{
			combined pretrained GANs for diverse full-body human image generation. Specialized GANs enabled seamless integration of parts which ensured  high-quality results. It relies however, on pretrained GANs, which potentially, might limits its adaptability to new domains. Also, coordinating multiple generators can be complex, requiring careful tuning and coordination.
		}


	\subsection{Multi-pose guided virtual try-on}
		These systems are a step up from pose-guided systems; given a input image of a person, the target clothing, and a target pose, these attempt to generate the person in the target clothing in the target pose.

        Generation of consecutive poses using \cite{DBLP:conf/icip/HsiehCCSC19} provides more information for the user to make more informed decisions while purchasing clothes. \cite{DBLP:conf/iccv/DongLSWLZH019} introduced a three-stage approach that addressed challenges such as self-occlusions, misalignment among diverse poses, and diverse clothes texture. The misalignment between the input human pose and the target pose is alleviated by the use of deep Warp-GAN.

        \lithead[3D MP-VTON]{DBLP:journals/access/ThaiMAW21}{
            allowed for accurate texture mapping, which is crucial for natural clothing rendering from arbitrary views and it greatly reduces segmentation label imbalance which results in high-quality segmentation and reduced training time.
        }

        Three submodules - semantic prediction module (SPM), clothes warping module (CWM), and try-on synthesis module (TSM) which worked together \cite{DBLP:journals/tmm/HuLZR22}  and generated visual try-on images with preserved clothing details and desired poses. \cite{du2023cf} also addressed the challenges of preserving the person's identity and unnatural garment alignment. The proposed model CF-VTON includes predicting the "after-try-on" semantic map, warping the garment using an improved GANet and synthesizing a coarse result with TSN.
        

	\subsection{Video virtual try-on}
		Video virtual try-on systems fit target clothes onto a person in a video with spatio-temporal consistency. This is challenging because usual image-based try-on methods cause frame-to-frame inconsistencies when applied to video data.

        \cite{DBLP:conf/wacv/KuppaJLLM21} through the use of ReLU and GeLU as the most effective activation functions, provided clarity on quantifying the isolated visual effect of different design choices. It also specified hyperparameter details for experimental reproduction. \lithead[MV-VTON]{DBLP:conf/mm/ZhongWTLW21}{
            transfers desired clothes to frame images through pose alignment and region-wise pixel replacement. It embeds generated frames into the latent space as external memory for subsequent frame generation.
        }

        \lithead[ClothFormer]{DBLP:conf/cvpr/JiangWYL22}{
            generates realistic, harmonious, and spatio-temporally consistent try-on videos in complicated environments. It predicts dense flow mapping between body and clothing regions which addresses the challenge of generating accurate warping when occlusions appear in the clothing region.
        }

	\subsection{3D virtual try-on}
		3D virtual try-on systems reconstruct 3D meshes of the person and clothing from the target images and then fit the clothing onto the person in attempts to generate a physically accurate 3D render.

        \lithead[ULNeF]{DBLP:conf/wacv/MajithiaPBGSS22}{
            allows for mix-and-match VTO, addressing the combinatorial complexity of mixing different garments. It works directly on neural implicit representations, which can bring a change of paradigm and open the door to radically different approaches in VTO. It has only been validated with garments in T-pose and lacks other variations.
        }

        The use of fixed topology parametric template mesh models by \cite{DBLP:conf/nips/SantestebanOTC22} for known types of garments allows for easy mapping of high-quality texture from input catalog images to UV map panels. The proposed pipeline is compact and scalable which makes it suitable for practical implementation.

	\subsection{Augmented reality try-on}
		Augmented reality virtual try-on is the eventual goal of all try-on systems, integrating computer-generated imagery with real-world views, enabling users to virtually try on clothing and accessories in real-time.
		
        The use of simple video dataset to successfully achieve transfer of clothing segmentation rather than using the DeepFashion dataset was proposed by \lithead{DBLP:conf/ieeehpcs/JongM19} {
            but the use of short video datasets may limit dataset size and diversity.
        }

		\cite{DBLP:journals/ijim/El-SeoudT19} provides the study of developing a Mobile Augmented Reality (MAR) application that helps store managers in sales strategy and proposing using of AR in market and helping customer in aiding decision making without physically wearing the clothes but it also bring the issue of long term viability and sustainability of such solution.

		\lithead{di2020comparative}{
			proposes the use of four neural models: Fully Connected Neural Network, CNN, MobileNetV1 \cite{DBLP:journals/corr/HowardZCKWWAA17}, and MobileNetV2 \cite{DBLP:conf/cvpr/SandlerHZZC18}, to classify clothing images for computationally limited platforms on which augmented reality is often implemented. Using MobileNetV2 improves the accuracy to 92.91\% but increases training time significantly.
		}

		\cite[]{hashmi2020augmented}{
			introduces a computationally inexpensive technique using web cam input and Haar cascade classifier \cite{DBLP:conf/cvpr/ViolaJ01} that helps avoid the use of costly kinect sensors. But the sample size used limits generalizability to broader populations. Also, Haar cascade classifier may have limitations in accurately detecting specific body parts, especially under varying lighting conditions and poses.
		}

        Providing the study of using AR to help online shoppers choose their correct size and help them choose the best option based upon various attributes Stimulus-Organism-Response model \cite{baytar2020evaluating} was used to investigate how AR products were perceived by consumers but the limited dataset reduces the generalizability of the findings. Another limitation was that the garments were 2D and did not wrap around the body.

        Advanced Virtual Apparel Try using Augmented Reality (AVATAR) \cite{shaw2020advanced} is capable of providing virtual apparel trial using AR. It provides a hardware solution using a multi-sensor body scanner for precise body coordinates. It also facilitates face color recognition, providing personalized apparel recommendations based on skin tone but no insights about real-world implementation challenges were provided and no information regarding accuracy and reliability of sensors was discussed.

		\cite[]{ali2021augmented} explores the use of smartphone camera to provide 3d image of product using AR which provides real time AR by using a camera and YOLO for detection but using YOLO introduces constraints on bounding box predictions, impacting small object detection and unusual configurations. Also, selective search in Faster R-CNN \cite{DBLP:journals/pami/RenHG017} can be time-consuming, affecting overall performance.

		\cite[]{feng2021personalized} proposes the use of AR, Azure Kinect somatosensory, and OpenGL 3D rendering to provide virtual try on experience and enhance clothing customization. \cite[]{moriuchi2021engagement} provides an insight about use of chatbots and AR technology in e-commerce. But the included study based upon small sample size of 68 millennials may limit generalizability. \cite[]{DBLP:journals/sensors/BattistoniGRSVB22} explores the role of AR as meta-user interface. It also covers case study on focus group that validates design patterns, providing insights for improvements.

	\subsection{Commercial uses of virtual try-on}
		Companies are also using virtual try-on systems to provide enhanced experience to customers. Table \ref{table:commercial-vton} lists the various commercially available virtual try-on platforms.

		\newcommand{\commrow}[3]{
			#2 & \cite{#1} & #3 \\ \addlinespace
		}

		\begin{table}[H]
			\caption{Commercial virtual try-on}
			\label{table:commercial-vton}
			\begin{tabularx}{\columnwidth}{BMX}
				\toprule
					\textbf{Technology} &
					\textbf{References} &
					\textbf{Limitations} \\
				\midrule
					\commrow{WalmartA, WalmartB, GoogleShopping}{2d (+ Pose)}{
						AI-generated variations of models wearing garments are not true representations of the user trying on the garment.
					}
					\commrow{Zalando, Snapchat, YTAR, BaumeMercier, LOreal, WarbyParker}{Augmented Reality}{
						Most implementations only focus on accessories like eyewear, wrist watches, or shoes, or makeup instead of actual clothing items.
					}
				\bottomrule
			\end{tabularx}
		\end{table}
