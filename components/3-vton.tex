\section{Virtual try-on} \label{section:vton}
	Virtual try-On systems represent a cutting-edge fusion of technology and fashion retail, revolutionizing the way consumers interact with clothing online. These systems utilize artificial intelligence, computer vision, and augmented reality to simulate the experience of trying on clothing virtually. Users can see how garments fit, drape, and look on their own bodies without physically trying them on. From 2D image-based try-ons to advanced 3D models, these systems offer a range of immersive experiences.
	
	The goal of virtual try-on is not only to enhance user confidence and satisfaction, but also to reduce return rates, making it a pivotal tool in modern e-commerce. It showcases the potential of technology to bridge the gap between digital and physical retail, offering consumers an engaging and informative way to make fashion choices online.

	\subsection{Image-based (2D) virtual try-on}
		Image-based try-on systems take in images as input data and generate images as output data. The systems infer data like pose, warp, and occlusion of the target and then use that information to guide the generation of the post-try-on image.

		\lithead[MGD]{DBLP:journals/corr/abs-2304-02051}{
			Incorporates multimodal inputs such as text, body pose, and sketches and outperforms other competitors (5.57 FID) in terms of realism and coherence with multimodal inputs. Also introduces new semi-automatically annotated datasets.
		}

		\lithead[GP-VTON]{DBLP:conf/cvpr/XieHDZDZ0L23}{
			Preserves semantic information, avoids texture distortion, and handles challenging inputs by employing local flows to warp garment parts individually and dynamically truncating the gradient in the overlap area, effectively avoiding texture squeezing problems. Easily extendeds to a multi-category scenarios. FID 11.98 and SSIM 0.94. However, computational requirements of the proposed techniques, Local-Flow Global-Parsing and Dynamic Gradient Truncation, are not discussed, and has a lack of evaluation on a wider range of datasets.
		}

		\lithead[SAL-VTON]{DBLP:conf/cvpr/YanGZX23}{
			Introduces semantically associated landmarks for virtual try-on which helps in addressing misalignment issues and improving the try-on results (FID 9.52 and SSIM 0.907). Also provides the ability to edit virtual try-on results using landmarks.
		}

		\lithead[TryOnDiffusion]{DBLP:conf/cvpr/ZhuYZRCS0K23}{
			Generates apparel try-on results with significant body shape and pose modification (FID 13.447). But focuses on upper body clothing and does not experiment with full body try-on. Further, performance with more complex backgrounds is unknown.
		}

		\lithead[3D-GCL]{DBLP:conf/nips/HuangLXKCL22}{
			Incorporates 3D parametric human models as priors to better handle variations of pose and viewpoint. Utilizes 3D-aware global correspondences that encode global semantic correlations. FID 10.58.
		}

		\lithead[C-VTON]{DBLP:conf/wacv/FeleLPS22}{
			Generates convincing results even with subjects in difficult poses. Allows for the synthesis of high-quality try-on results. FID 19.54. But has issues with the masking procedure when generating image context and loose clothing. Further, lacks the ability to differentiate between the front and backside of the target garment.
		}

		\lithead[DressCode]{DBLP:conf/cvpr/MorelliFCLCC22}{
			Makes predictions at the pixel-level, resulting in high visual quality and rich in detail try-on images. FID 11.4. But is limitated in terms of representing all possible clothing categories and styles. Also, does not explicitly address issues such as occlusion, lighting variations, or realistic fabric rendering.
		}

		\lithead[HR-VTON]{DBLP:conf/eccv/LeeGPCC22}{
			Ensures information exchange and eliminates misalignment and pixel-squeezing artifacts by filtering out incorrect segmentation map predictions. FID 10.91.
		}

		\lithead[SDAFAN]{DBLP:conf/eccv/BaiZLZY22}{
			Enables clothing warping and body synthesizing simultaneously by extracting and merging feature and pixel-level information from different semantic areas. Addresses challenges without relying on intermediate parser-based labels, reducing noise and inaccuracies in the try-on process. FID 10.97. But handling of complex clothing patterns or textures is a limitation.
		}

		\lithead[RT-VTON]{DBLP:conf/cvpr/YangY022}{
			Can accurately synthesize photo-realistic results for both standard and non-standard clothes. Is able to handle hard samples such as off-shoulder clothes. Semi-rigid deformation technique used in the method balances the trade-off between rigidity and flexibility of clothes warping. FID 11.66.
		}

		\lithead{DBLP:conf/cvpr/HeSX22}{
			Proposes a perceptual loss and a warping model to train the try-on model, which helps improve the quality of the generated appearance flow and the final try-on results. Usage of a pre-trained VGG network and a parser-based model in the training process enhances the accuracy and effectiveness (FID 8.89).
		}

		\lithead{DBLP:conf/cvpr/FengMSGLLOZZ22}{
			Proposes a cheap and scalable weakly-supervised method and shows that projecting the rough alignment of clothing and body onto the StyleGAN space can yield photo-realistic wearing results. The Deep Generative Projection algorithm has a time cost for semantic and pattern searches and the method still faces difficulty against state-of-the-art methods in handling extremely complicated poses. Has an undesirably high FID score of 48.4.
		}

		\lithead[wFlow]{DBLP:conf/cvpr/DongZXZDZLLY22}{
			Is capable of transferring arbitrary garments onto challengingly-posed query person images in real-world backgrounds. Efficiently integrates the advantages of 2D pixel-flow and 3D vertex-flow. The self-supervised training scheme used in the paper leverages easily obtainable dance videos to train the model. FID 8.89. However, since it focuses on garment transfer in the context of dance videos, it is unclear how well it performs on a large scale or with diverse datasets.
		}

		\lithead[DBCT]{DBLP:conf/cvpr/FenocchiMCBCC22}{
			Improves the generation of virtual try-on results by dealing with long-range dependencies and achieving more realistic and accurate results and demonstrates its effectiveness in comparison to both standard pure convolutional approaches and previous Transformer-based proposals. FID 13.46.
		}

		\lithead[DiOr]{DBLP:conf/iccv/CuiML21}{
			Supports various fashion-related tasks such as 2D pose transfer, virtual try-on, and outfit editing. Explicitly encodes the shape and texture of each garment, allowing for separate editing of these elements. Joint training on pose transfer and inpainting helps with detail preservation and coherence of generated garments. FID 13.1. However, complex or rarely seen poses may not always be rendered correctly, and ghosting artifacts and improper filling of holes in garments can also be present.
		}

		\lithead[ZFlow]{DBLP:conf/iccv/ChopraJHK21}{
			Proposes an end-to-end framework for image-based virtual try-on addressing concerns regarding geometric and textural integrity. Incorporates gated aggregation of hierarchical flow estimates, termed Gated Appearance Flow, and dense structural priors to improve depth perception, handling of occlusion, and reduction of artifacts in try-on outputs. FID 15.17.
		}

		\lithead[FashionMirror]{DBLP:conf/iccv/ChenLHSC21}{
			Proposes a co-attention feature-remapping framework for virtual try-on to generate realistic results with spatio-temporal smoothness. Allows for refinement through convolution and generating different views. FID 5.226. However, does not address high-resolution virtual try-on, and does not provide a comprehensive analysis of the computational efficiency or scalability of the proposed framework.
		}

		\lithead[PF-AFN]{DBLP:conf/cvpr/GeSZG0021}{
			Produces highly photo-realistic try-on images without relying on human parsing by the use of appearance flows between the person image and the garment image which enables accurate dense correspondences and high-quality results. Introduces an adjustable distillation loss function to ensure accurate representations and predictions. Evaluations show the large superiority of the proposed method in terms of image quality compared to existing approaches. FID 6.429. However, heavily relies on the use of a parser-based model as the "teacher" network, which may limit the image quality of the "student" network.
		}

		\lithead[VITON-HD]{DBLP:conf/cvpr/ChoiPLC21}{
			Successfully synthesizes high-resolution virtual try-on images. Introduces a misalignment-aware normalization technique to address spatial deformation and generate properly aligned images. Utilizes conditional normalization layers to preserve semantic information and apply spatially varying affine transformations. FID 11.74.
		}

		\lithead[DCTON]{DBLP:conf/cvpr/GeSGY0021}{
			Can produce highly-realistic try-on images by disentangling important components of virtual try-on. Utilizes perceptual loss to ensure similar CNN feature representations between the warped clothes. Disentangles clothing, skin, and background components from input images. FID 14.82. However, does not differentiate clothing and non-clothing regions, which can hinder the quality of results.
		}

		\lithead[OVNet]{DBLP:conf/cvpr/LiCZL21}{
			Captures important details like buttons, shading, textures, and realistic hemlines, resulting in high-quality virtual try-on images. Introduces a technique for matching outfits with the most suitable model, leading to significant improvements in try-on results. Also consists of a semantic layout generator and an image generation pipeline using multiple coordinated warps, which yields consistent improvements in detail (FID 7.02). But does not handle variations in body shape, which limits its ability to dress garments directly on different body types. It also does not address the challenge of obtaining 3D measurements of garments and users.
		}

	\subsection{Pose-guided human synthesis}
		These systems are also image-based, but they focus on transforming a human image from reference to target pose while preserving style but changing clothing.

	\subsection{Multi-pose guided virtual try-on}
		These systems are a step up from pose-guided systems; given a input image of a person, the target clothing, and a target pose, these attempt to generate the person in the target clothing in the target pose.

        \lithead[FIT-ME]{DBLP:conf/icip/HsiehCCSC19}{
            Allows for the generation of consecutive poses, providing users with more information to make informed decisions about purchasing clothes.
        }

        \lithead[MG-VTON]{DBLP:conf/iccv/DongLSWLZH019}{
            Introduces a three-stage approach that addresses challenges such as self-occlusions, misalignment among diverse poses, and diverse clothes texture. IS 2.03 and SSIM 0.744. Use of a deep Warp-GAN helps alleviate the misalignment problem between the input human pose and desired human pose.
        }

        \lithead[3D MP-VTON]{DBLP:journals/access/ThaiMAW21}{
            Allows for accurate texture mapping, which is crucial for natural clothing rendering from arbitrary views. FID 13.715. Greatly reduces segmentation label imbalance, resulting in high-quality segmentation and reduced training time.
        }

        \lithead[SPG-VTON]{DBLP:journals/tmm/HuLZR22}{
            Introduces three submodules - semantic prediction module (SPM), clothes warping module (CWM), and try-on synthesis module (TSM) - that work together to generate virtual try-on images with preserved clothing details and desired poses. IS 3.14 and SSIM 0.752.
        }

        \lithead[CF-VTON]{du2023cf}{
            Addresses challenges of unnatural garment alignment and difficulty in preserving the person's identity. FID 12.38. Includes predicting the "after-try-on" semantic map, warping the garment using an improved GANet, synthesizing a coarse result with TSN.
        }

	\subsection{Video virtual try-on}
		Video virtual try-on systems fit target clothes onto a person in a video with spatio-temporal consistency. This is challenging because usual image-based try-on methods cause frame-to-frame inconsistencies when applied to video data.

        \lithead[ShineOn]{DBLP:conf/wacv/KuppaJLLM21}{
            Provides clarity on quantifying the isolated visual effect of different design choices and specifies key hyperparameter details for experimental reproduction. SSIM 0.94. Demonstrates that GELU and ReLU activation functions are the most effective in the experiments.
        }

        \lithead[MV-VTON]{DBLP:conf/mm/ZhongWTLW21}{
            Transfers desired clothes to frame images through pose alignment and region-wise pixel replacement. Embeds generated frames into the latent space as external memory for subsequent frame generation.
        }

        \lithead[ClothFormer]{DBLP:conf/cvpr/JiangWYL22}{
            Generates realistic, harmonious, and spatio-temporally consistent try-on videos in complicated environments. VFID 3.967 and SSIM 0.921. Predicts dense flow mapping between body and clothing regions, addressing the challenge of generating accurate warping when occlusions appear in the clothing region
        }

	\subsection{3D virtual try-on}
		3D virtual try-on systems reconstruct 3D meshes of the person and clothing from the target images and then fit the clothing onto the person in attempts to generate a physically accurate 3D render.

        \lithead[ULNeF]{DBLP:conf/wacv/MajithiaPBGSS22}{
            Allows for mix-and-match VTO, addressing the combinatorial complexity of mixing different garments. Works directly on neural implicit representations, which can bring a change of paradigm and open the door to radically different approaches in VTO. Has only been validated with garments in T-pose and lacks other variations.
        }

        \lithead[3D garment digitization]{DBLP:conf/nips/SantestebanOTC22}{
            Use of fixed topology parametric template mesh models for known types of garments allows for easy mapping of high-quality texture from input catalog images to UV map panels. SSIM 0.977. Proposed pipeline is compact and scalable, making it suitable for practical implementation.
        }

	\subsection{Augmented reality try-on}
		Augmented reality virtual try-on is the eventual goal of all try-on systems, integrating computer-generated imagery with real-world views, enabling users to virtually try on clothing and accessories in real-time.
		
	\subsection{Commercial uses of virtual try-on}
		Companies are also using virtual try-on systems to provide enhanced experience to customers. Though many of them are limited to items like accessories and make-up, clothing try-ons are not far away.
