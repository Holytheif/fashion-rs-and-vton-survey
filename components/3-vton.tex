\section{Virtual try-on} \label{section:vton}
	Virtual try-On systems represent a cutting-edge fusion of technology and fashion retail, revolutionizing the way consumers interact with clothing online. These systems utilize artificial intelligence, computer vision, and augmented reality to simulate the experience of trying on clothing virtually. Users can see how garments fit, drape, and look on their own bodies without physically trying them on. From 2D image-based try-ons to advanced 3D models, these systems offer a range of immersive experiences.
	
	The goal of virtual try-on is not only to enhance user confidence and satisfaction, but also to reduce return rates, making it a pivotal tool in modern e-commerce. It showcases the potential of technology to bridge the gap between digital and physical retail, offering consumers an engaging and informative way to make fashion choices online.

	\subsection{Image-based (2D) virtual try-on}
		Image-based try-on systems take in images as input data and generate images as output data. The systems infer data like pose, warp, and occlusion of the target and then use that information to guide the generation of the post-try-on image.

		In \citeyear{DBLP:conf/iccvw/AyushJCHK19} \lithead{DBLP:conf/iccvw/AyushJCHK19}{
			present a multi-scale patch adversarial loss function that improves cloth fit and texture preservation in virtual try-on. A Geometric Matching Module (GMM) enhances control over shape and pose transformations, and adversarial loss promotes realistic texture and body shape propagation. Reformulation as a conditional image generation problem offers an economical virtual try-on solution without 3D information.
		} In another work \cite{DBLP:conf/iccvw/AyushJCK19}, they demonstrate how multi-task learning improves efficiency and task performance. Auxiliary tasks like semantic segmentation can enhance virtual try-on, and pseudo ground-truth masks aid in image fusion, leading to a better fit.
		
		\lithead[ClothFlow]{DBLP:conf/iccv/HanHHS19}{
			utilizes dense optical flow estimation for natural-looking clothing deformation and appearance transfer. The cascaded flow network ensures accurate appearance matching, enhancing clothed person generation and a conditional layout generator disentangles shape and appearance for spatially coherent results.
		} And in a similar vein, \lithead[UVTON]{DBLP:conf/iccvw/KuboISM19}{
			leverages UV mapping for accurate virtual try-on across diverse postures.
		} Both of these rely on DensePose integrations to enhance mapping precision by estimating 3D surface information for each pixel. However, DensePose accuracy is crucial for success and that is challenging as warping 2D image textures to predefined coordinate systems can introduce artifacts.

		\lithead[LA-VITON]{DBLP:conf/iccvw/LeeLKCP19}{
			excels in damage-free, visually appealing virtual try-on results by using a GMM and Try-On Module (TOM) to ensure seamless synthesis. A Grid interval consistency loss maintains shape and pattern consistency, and an occlusion-handling technique enhances geometric matching for superior results.
		} And \lithead[VTNFP]{DBLP:conf/iccv/YuWX19}{
			retains clothing and body details by leveraging a GAN \cite{DBLP:journals/corr/GoodfellowPMXWOCB14} and cGAN's \cite{DBLP:journals/corr/MirzaO14} powerful generative capabilities.
		}

		The use of StyleGAN \cite{DBLP:journals/pami/KarrasLA21} by \lithead{DBLP:conf/iccvw/YildirimJVB19}{
			enables high-resolution fashion model image generation, enhancing clothing shopping visualizations. Conditional StyleGAN with embedding network allows customization of outfits and poses for fashion models. Swapping style vectors in unconditional StyleGAN enhances flexibility in image generation. Further, deep pose estimator and keypoint extraction improve accuracy in representing body poses.
		}

		In \citeyear{minar2020cp} \lithead[CP-VTON+]{minar2020cp}{
			corrects clothing-agnostic human representation by addressing labeling and omission issues. It enhances composition masks using input clothing mask and a concrete loss function.
		}

		\lithead[WUTON]{DBLP:conf/eccv/IssenhuthMC20}{
			adopts a student-teacher paradigm to improve virtual try-on performance by exploiting masked information. Adversarial loss ensures the student model mimics real image distribution, enhancing realism. A Real-time virtual try-on is achieved by omitting human parser and pose estimator at inference.
		}

		A hybrid 2D-3D method proposed by \lithead{minar20203d}{
			enables accurate 3D clothing reconstruction for natural shapes. 3D deformations preserve clothing characteristics, especially for large transformations or detailed textures, and the template-based approach aligns clothing image with the silhouette of the body's 3D model for precise reconstruction. SMPL model ensures accurate representation of diverse body shapes and poses. An updated TOM network enhances blending stage for improved virtual try-on results.
		}

		In \citeyear{DBLP:conf/cvpr/LiCZL21} \lithead[OVNet]{DBLP:conf/cvpr/LiCZL21}{
			presents a method to capture important details like buttons, shading, textures, and realistic hemlines, resulting in high-quality virtual try-on images. It introduces a technique for matching outfits with the most suitable model, leading to significant improvements in try-on results. It also consists of a semantic layout generator and an image generation pipeline using multiple coordinated warps, which yields consistent improvements in detail. But it does not handle variations in body shape, which limits its ability to dress garments directly on different body types. It also does not address the challenge of obtaining 3D measurements of garments and users.
		}

		Yet another hybrid approach is proposed in \lithead[CloTH-VTON+]{DBLP:journals/access/MinarTA21}{
			which combines 2D and 3D methods for realistic clothing deformations while preserving details. 2D methods generate disclosed human parts, ensuring visually appealing try-on outputs, while 3D cloth reconstruction allows flexibility in applying the method to different clothing styles.
		}

		\lithead[DCTON]{DBLP:conf/cvpr/GeSGY0021}{
			can produce highly-realistic try-on images by disentangling important components of virtual try-on. It utilizes perceptual loss to ensure similar CNN feature representations between the warped clothes. It disentangles clothing, skin, and background components from input images. However, it does not differentiate clothing and non-clothing regions, which can hinder the quality of results.
		}

		Successful synthesis of high-resolution virtual try-on is achieved by \lithead[VITON-HD]{DBLP:conf/cvpr/ChoiPLC21}{} It introduces a misalignment-aware normalization technique to address spatial deformation and generate properly aligned images and utilizes conditional normalization layers to preserve semantic information and apply spatially varying affine transformations.

		Another student-teacher model, \lithead[PF-AFN]{DBLP:conf/cvpr/GeSZG0021}{
			produces highly photo-realistic try-on images without relying on human parsing by the use of appearance flows between the person image and the garment image which enables accurate dense correspondences and high-quality results. It introduces an adjustable distillation loss function to ensure accurate representations and predictions. However, heavily relies on the use of a parser-based model as the `teacher' network, which may limit the image quality of the `student' network.
		}

		\lithead[FashionMirror]{DBLP:conf/iccv/ChenLHSC21}{
			proposes a co-attention feature remapping framework for virtual try-on to generate realistic results with spatio-temporal smoothness. It allows for refinement through convolution and generating different views.
		} \lithead[ZFlow]{DBLP:conf/iccv/ChopraJHK21}{
			presents an end-to-end framework addressing concerns regarding geometric and textural integrity. It incorporates gated aggregation of hierarchical flow estimates (Gated Appearance Flow), and dense structural priors to improve depth perception, handling of occlusion, and reduction of artifacts in try-on outputs.
		}

		\lithead[DiOr]{DBLP:conf/iccv/CuiML21}{
			supports various fashion-related tasks such as 2D pose transfer, virtual try-on, and outfit editing. It explicitly encodes the shape and texture of each garment, allowing for separate editing of these elements. A joint training on pose transfer and inpainting helps with detail preservation and coherence of generated garments. However, complex or rarely seen poses may not always be rendered correctly, and ghosting artifacts and improper filling of holes in garments can also be present.
		}

		In \citeyear{DBLP:conf/cvpr/FenocchiMCBCC22} \lithead[DBCT]{DBLP:conf/cvpr/FenocchiMCBCC22}{
			improves the generation of virtual try-on results by dealing with long-range dependencies and achieving more realistic and accurate results and demonstrates its effectiveness in comparison to both standard pure convolutional approaches and previous transformer-based proposals.
		}

		A cheap and scalable weakly-supervised method is proposed by \lithead{DBLP:conf/cvpr/FengMSGLLOZZ22}{
			which shows that projecting the rough alignment of clothing and body onto the StyleGAN space can yield photo-realistic wearing results. The Deep Generative Projection algorithm has a time cost for semantic and pattern searches and the method still faces difficulty against state-of-the-art methods in handling extremely complicated poses. Has an undesirably high FID score of 48.4.
		}

		\lithead{DBLP:conf/cvpr/HeSX22}{
			presents a perceptual loss and a warping model to train the try-on model, which helps improve the quality of the generated appearance flow and the final try-on results. Usage of a pre-trained VGG network \cite{DBLP:journals/corr/SimonyanZ14a} and a parser-based model in the training process enhances the accuracy and effectiveness.
		}

		Accurate photorealistic results are shown by \lithead[RT-VTON]{DBLP:conf/cvpr/YangY022}{
			for both standard and non-standard clothes. Is able to handle hard samples such as off-shoulder clothes. Semi-rigid deformation technique used in the method balances the trade-off between rigidity and flexibility of clothes warping.
		}

		\lithead[SDAFAN]{DBLP:conf/eccv/BaiZLZY22}{
			enables clothing warping and body synthesizing simultaneously by extracting and merging feature and pixel-level information from different semantic areas. It addresses challenges without relying on intermediate parser-based labels, reducing noise and inaccuracies in the try-on process.
		} \lithead[HR-VTON]{DBLP:conf/eccv/LeeGPCC22}{
			ensures information exchange and eliminates misalignment and pixel-squeezing artifacts by filtering out incorrect segmentation map predictions.
		}

		\lithead[C-VTON]{DBLP:conf/wacv/FeleLPS22}{
			generates convincing results even with subjects in difficult poses, allowing for synthesis of high-quality try-on results. However, it has issues with the masking procedure when generating image context and loose clothing. Further, it lacks the ability to differentiate between the front and backside of the target garment.
		} \lithead[3D-GCL]{DBLP:conf/nips/HuangLXKCL22}{
			incorporates 3D parametric human models as priors to better handle variations of pose and viewpoint. It utilizes 3D-aware global correspondences that encode global semantic correlations.
		}

		In \citeyear{DBLP:conf/cvpr/ZhuYZRCS0K23} \lithead[TryOnDiffusion]{DBLP:conf/cvpr/ZhuYZRCS0K23}{
			generates apparel try-on results with significant body shape and pose modification by using a latent diffusion method \cite{DBLP:conf/cvpr/RombachBLEO22}. But it focuses on upper body clothing and does not experiment with full body try-on. Further, performance with more complex backgrounds is unknown.
		} \lithead[MGD]{DBLP:journals/corr/abs-2304-02051}{
			also takes this diffusion model approach and incorporates multimodal inputs such as text, body pose, and sketches and achieves good results in terms of realism and coherence with multimodal inputs. It also introduces new semi-automatically annotated datasets.
		}

		\lithead[SAL-VTON]{DBLP:conf/cvpr/YanGZX23}{
			introduces semantically associated landmarks for virtual try-on which helps in addressing misalignment issues and improving the try-on results. It also provides the ability to edit virtual try-on results using landmarks.
		} \lithead[GP-VTON]{DBLP:conf/cvpr/XieHDZDZ0L23}{
			preserves semantic information, avoids texture distortion, and handles challenging inputs by employing local flows to warp garment parts individually and dynamically truncating the gradient in the overlap area, effectively avoiding texture squeezing problems. It also easily extendeds to a multi-category scenarios.
		}

	\subsection{Pose-guided human synthesis}
		These systems are also image-based, but they focus on transforming a human image from reference to target pose while preserving style but changing clothing.

		\lithead{DBLP:conf/cvpr/SongZLM19}{
			Proposed method simplifies non-rigid deformation learning through semantic parsing transformation and appearance generation tasks. Semantic generative network aids in transforming semantic parsing maps, facilitating deformation learning. Appearance generative network synthesizes semantic-aware textures for realistic person images. End-to-end training refines results, leading to improved performance. SSIM 0.736.
		}
		
		\lithead{DBLP:conf/iccv/AlbaharH19}{
			Proposes a conditioning scheme for controlled image synthesis with diverse guidance signals and a bi-directional feature transformation for enhanced information flow. Applicable to tasks like person image synthesis, sketch-to-photo, and depth upsampling. SSIM 0.767.
		}

		\lithead{DBLP:conf/cvpr/LiHL19}{
			3D appearance flow enables precise pixel transfer for realistic pose transformation. Scarce annotations are overcome by fitting a 3D model and projecting to 2D for dense appearance flow computation. Synthesized ground-truths efficiently map input and target poses to 3D appearance flow for accurate pose transfer. Feature warping with appearance flow creates photorealistic target pose images. SSIM 0.778.
		}
		
		\lithead{DBLP:journals/corr/abs-1906-07251}{
			Two discriminators differentiate between real and generated images, ensuring fashion item consistency in synthesis. Image and pose encoders provide crucial context for high-quality image generation. End-to-end training optimizes generative model parameters for photorealistic image synthesis. SSIM 0.789.
		}

		\lithead{DBLP:conf/iccv/LiuPML0G19}{
			3D body mesh recovery module enables pose and shape disentanglement for personalized modeling. Liquid Warping GAN with Liquid Warping Block preserves vital details in both image and feature spaces. Supports flexible warping from multiple sources, yielding diverse results. SSIM 0.84.
		}

		\lithead[APS]{DBLP:conf/ijcai/HuangXCWZWHD20}{
			Couples target pose with conditioned appearance for realistic person image generation. Introduction of adaptive patch normalization improves performance through region-specific normalization. Decoupling and re-coupling of factors offer high flexibility in generating person images. SSIM 0.775.
		}

		\lithead{DBLP:conf/cvpr/MenMJML20}{
			Attribute-Decomposed GAN enables precise attribute control in image synthesis. Automatic separation of attributes eliminates manual annotation, enhancing realism. SSIM 0.772. However, dependency on labeled data may limit applicability in scenarios lacking readily available annotations.
		}
		
		\lithead[PoNA]{DBLP:journals/tip/LiZLLD20}{
			GANs enhance image quality for realistic pose transfer results. Pose-Guided Non-Local Attention (PoNA) mechanism with long-range dependency addresses feature information gaps and self-occlusion issues. Integration of pre-posed image-guided pose feature update and post-posed image feature update improves feature utilization in the transfer process. Simplified cascaded blocks in the generator ensure network simplicity, stability, and ease of training. Method generates sharper, detailed images with fewer parameters and faster speed, increasing efficiency for pose transfer tasks. SSIM 0.775. However, pose-guided attention struggles with invisible areas, posing limitations in occlusion handling. Simplified block structure may struggle with complex poses, impacting result accuracy. GAN reliance may lead to mode collapse or instability, affecting image quality and diversity. Difficulty in preserving fine details and accurately transferring textures, especially with significant coordinate differences.
		}

		\lithead[SPAdaIN]{DBLP:conf/cvpr/WangWFLZXZ20}{
			Works directly on identity meshes, bypassing need for specific additional information. 3D SPAdaIN extends successful 2D SPADE \cite{DBLP:conf/cvpr/Park0WZ19} approach, enabling effective pose transfer. SPAdaIN ResBlocks ensure high-quality output meshes with combined pose and identity information. Edge regularization maintains geometric details for enhanced results. However, dependency on high-quality identity meshes and pose information may hinder practical use in some scenarios.
		}

		\lithead{DBLP:journals/tip/RenLLL20}{
			Enables accurate feature-level spatial transformations, enhancing pose-guided image generation. Combination of flow-based operations and attention improves training stability and gradient propagation. FID 10.573. However, has difficulty excluding irrelevant info which affects texture accuracy in the global attention model.
		}
		
		\lithead{DBLP:journals/corr/abs-2008-11898}{
			Enables high-resolution appearance transfer without 3D model, enhancing accessibility and efficiency. Utilizes dense local descriptors for refined details and preserved garment textures and geometry. Progressive training in autoencoder improves generation quality at high resolutions. Realistically reproduces complex garment appearance, including occluded areas, for accurate transfer. Experimental results demonstrate potential for applications like garment transfer and pose-guided video generation. SSIM 0.806. However, global perceptual loss may not preserve sharp garment textures, as noted in ablation study. Computational complexity and efficiency not discussed, potentially impacting practical use. SSIM 0.806.
		}

		\lithead{DBLP:journals/corr/abs-2006-01435}{
			Simplifies portrait editing, eliminating manual software use, allows simultaneous revision of posture, body figure, and clothing style by using a GAN-based method which maintains coherency and preserves identity in recaptured portraits. Layout map guides appearance transformation, enhancing accuracy. Hierarchical knowledge aids in effective recapture, especially for invisible parts. FID 15.936 and SSIM 0.778.
		}

		\lithead{DBLP:journals/tog/AlBaharLYSSH21}{
			Conditional StyleGAN enables accurate pose-guided image synthesis. Inpainted Correspondence Field facilitates detail transfer for drastic pose changes. Spatially Varying Latent Space Modulation preserves local details and photo-realism. FID 6.0557 and SSIM 0.7711.
		}

		\lithead[PATN]{DBLP:journals/pami/ZhuHXSCB22}{
			GANs enhance image realism and shape consistency, yielding visually appealing results. Pose-Attentional Transfer Netowrk allows accurate and natural-looking pose transfers through intermediate representations. Perceptual L1 loss integration reduces pose distortions, enhancing image quality. SSIM 0.773. However, limited diversity in training data may affect real-world adaptability.
		}

		\lithead[wFlow]{DBLP:conf/cvpr/DongZXZDZLLY22}{
			Is capable of transferring arbitrary garments onto challengingly-posed query person images in real-world backgrounds. Efficiently integrates the advantages of 2D pixel-flow and 3D vertex-flow. The self-supervised training scheme used in the paper leverages easily obtainable dance videos to train the model. FID 8.89. However, since it focuses on garment transfer in the context of dance videos, it is unclear how well it performs on a large scale or with diverse datasets.
		}

		\lithead[CASD]{DBLP:conf/eccv/ZhouYCSGL22}{
			Enables controlled person image synthesis by blending source style with target pose. Self-attention aids accurate source appearance encoding, enhancing model effectiveness. Contextual loss, computed using VGG-19 features, facilitates image transformation. FID 11.3732 and SSIM 0.7248.
		}

		\lithead[InsetGAN]{DBLP:conf/cvpr/FruhstuckSSMWL22}{
			Combines pretrained GANs for diverse full-body human image generation. Specialized GANs enable seamless integration of parts, ensuring high-quality results. Relies however, on pretrained GANs, potentially limiting adaptability to new domains. Coordinating multiple generators can be complex, requiring careful tuning and coordination.
		}


	\subsection{Multi-pose guided virtual try-on}
		These systems are a step up from pose-guided systems; given a input image of a person, the target clothing, and a target pose, these attempt to generate the person in the target clothing in the target pose.

        \lithead[FIT-ME]{DBLP:conf/icip/HsiehCCSC19}{
            Allows for the generation of consecutive poses, providing users with more information to make informed decisions about purchasing clothes.
        }

        \lithead[MG-VTON]{DBLP:conf/iccv/DongLSWLZH019}{
            Introduces a three-stage approach that addresses challenges such as self-occlusions, misalignment among diverse poses, and diverse clothes texture. IS 2.03 and SSIM 0.744. Use of a deep Warp-GAN helps alleviate the misalignment problem between the input human pose and desired human pose.
        }

        \lithead[3D MP-VTON]{DBLP:journals/access/ThaiMAW21}{
            Allows for accurate texture mapping, which is crucial for natural clothing rendering from arbitrary views. FID 13.715. Greatly reduces segmentation label imbalance, resulting in high-quality segmentation and reduced training time.
        }

        \lithead[SPG-VTON]{DBLP:journals/tmm/HuLZR22}{
            Introduces three submodules - semantic prediction module (SPM), clothes warping module (CWM), and try-on synthesis module (TSM) - that work together to generate virtual try-on images with preserved clothing details and desired poses. IS 3.14 and SSIM 0.752.
        }

        \lithead[CF-VTON]{du2023cf}{
            Addresses challenges of unnatural garment alignment and difficulty in preserving the person's identity. FID 12.38. Includes predicting the "after-try-on" semantic map, warping the garment using an improved GANet, synthesizing a coarse result with TSN.
        }

	\subsection{Video virtual try-on}
		Video virtual try-on systems fit target clothes onto a person in a video with spatio-temporal consistency. This is challenging because usual image-based try-on methods cause frame-to-frame inconsistencies when applied to video data.

        \lithead[ShineOn]{DBLP:conf/wacv/KuppaJLLM21}{
            Provides clarity on quantifying the isolated visual effect of different design choices and specifies key hyperparameter details for experimental reproduction. SSIM 0.94. Demonstrates that GELU and ReLU activation functions are the most effective in the experiments.
        }

        \lithead[MV-VTON]{DBLP:conf/mm/ZhongWTLW21}{
            Transfers desired clothes to frame images through pose alignment and region-wise pixel replacement. Embeds generated frames into the latent space as external memory for subsequent frame generation.
        }

        \lithead[ClothFormer]{DBLP:conf/cvpr/JiangWYL22}{
            Generates realistic, harmonious, and spatio-temporally consistent try-on videos in complicated environments. VFID 3.967 and SSIM 0.921. Predicts dense flow mapping between body and clothing regions, addressing the challenge of generating accurate warping when occlusions appear in the clothing region
        }

	\subsection{3D virtual try-on}
		3D virtual try-on systems reconstruct 3D meshes of the person and clothing from the target images and then fit the clothing onto the person in attempts to generate a physically accurate 3D render.

        \lithead[ULNeF]{DBLP:conf/wacv/MajithiaPBGSS22}{
            Allows for mix-and-match VTO, addressing the combinatorial complexity of mixing different garments. Works directly on neural implicit representations, which can bring a change of paradigm and open the door to radically different approaches in VTO. Has only been validated with garments in T-pose and lacks other variations.
        }

        \lithead{DBLP:conf/nips/SantestebanOTC22}{
            Use of fixed topology parametric template mesh models for known types of garments allows for easy mapping of high-quality texture from input catalog images to UV map panels. SSIM 0.977. Proposed pipeline is compact and scalable, making it suitable for practical implementation.
        }

	\subsection{Augmented reality try-on}
		Augmented reality virtual try-on is the eventual goal of all try-on systems, integrating computer-generated imagery with real-world views, enabling users to virtually try on clothing and accessories in real-time.
		
		\lithead{DBLP:conf/ieeehpcs/JongM19}{
			Suggests the use of simple to collect video dataset that is sufficient to successfully achieve transfer of clothing segmentations rather than using the DeepFashion dataset. But using short video datasets may limit dataset size and diversity.
		}

		\lithead{DBLP:journals/ijim/El-SeoudT19}{
			Provides the study of developing a Mobile Augmented Reality (MAR) application that helps store managers help in sales strategy and proposing using of AR in market and helping customer in aiding decision making without physically wearing the clothes. But it also bring the issue of long term viability and sustainabilty of such solution.
		}

		\lithead{di2020comparative}{
			Proposes the use of four neural models: Fully Connected Neural Network, CNN, MobileNetV1 \cite{DBLP:journals/corr/HowardZCKWWAA17}, and MobileNetV2 \cite{DBLP:conf/cvpr/SandlerHZZC18}, to classify clothing images for computationally limited platforms on which augmented reality is often implemented. Using MobileNetV2 improves the accuracy to 92.91\% but increases training time significantly.
		}

		\lithead[]{hashmi2020augmented}{
			Introduces a computationally inexpensive technique using web cam input and Haar cascade classifier \cite{DBLP:conf/cvpr/ViolaJ01} that helps avoid the use of costly kinect sensors. But the sample size used limits generalizability to broader populations. Also, Haar cascade classifier may have limitations in accurately detecting specific body parts, especially under varying lighting conditions and poses.
		}

		\lithead[]{baytar2020evaluating}{
			Provides the study of using AR to help online shoppers choose their correct size and help them choose the best option based upon various attributes. Stimulus-Organism-Response model was used to investigate how AR products were percieved by consumers. But the limited dataset reduces the generalizability of the findings. Another limitation was that the garments were 2D and did not wrap around the body.
		}

		\lithead[AVATAR]{shaw2020advanced}{
			Is capable of providing virtual apparel trial using AR. Advanced Virtual Apparel Try using Augmented Reality (AVATAR) provides a hardware solution using a multi-sensor body scanner for precise body coordinates. It also facilitates face color recognition, providing personalized apparel recommendations based on skin tone. But no insights about real-world implementation challenges were provided and no information regarding accuracy and reliability of sensors was discussed.
		}

		\lithead[]{ali2021augmented}{
			Explores the use of smartphone camera to provide 3d image of product using AR which provides real time AR by using a camera and YOLO for detection. But using YOLO introduces constraints on bounding box predictions, impacting small object detection and unusual configurations. Also, selective search in Faster R-CNN \cite{DBLP:journals/pami/RenHG017} can be time-consuming, affecting overall performance.
		}

		\lithead[]{feng2021personalized}{
			Proposes the use of AR, Azure Kinect somatosensory, and OpenGL 3D rendering to provide virtual try on experience and enhance clothing customization.
		}

		\lithead[]{moriuchi2021engagement}{
			Provides an insight about use of chatbots and AR technology in e-commerce. But the included study based upon small sample size of 68 millennials may limit generalizability.
		}
		
		\lithead[]{DBLP:journals/sensors/BattistoniGRSVB22}{
			Explores the role of AR as meta-user interface. It also covers case study on focus group that validates design patterns, providing insights for improvements.
		}

	\subsection{Commercial uses of virtual try-on}
		Companies are also using virtual try-on systems to provide enhanced experience to customers. Table \ref{table:commercial-vton} lists the various commercially available virtual try-on platforms.

		\newcommand{\commrow}[3]{
			#2 & \cite{#1} & #3 \\ \addlinespace
		}

		\begin{table}[H]
			\caption{Commercial virtual try-on}
			\label{table:commercial-vton}
			\begin{tabularx}{\columnwidth}{BMX}
				\toprule
					\textbf{Technology} &
					\textbf{References} &
					\textbf{Limitations} \\
				\midrule
					\commrow{WalmartA, WalmartB, GoogleShopping}{2d (+ Pose)}{
						AI-generated variations of models wearing garments are not true representations of the user trying on the garment.
					}
					\commrow{Zalando, Snapchat, YTAR, BaumeMercier, LOreal, WarbyParker}{Augmented Reality}{
						Most implementations only focus on accessories like eyewear, wrist watches, or shoes, or makeup instead of actual clothing items.
					}
				\bottomrule
			\end{tabularx}
		\end{table}
